{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d8f86",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision \n",
    "!pip install transformers datasets tqdm ipywidgets hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U torch torchvision \n",
    "%pip install transformers datasets tqdm ipywidgets hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPVisionModel\n",
    "from datasets import load_from_disk\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a4e86",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f522f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "reference = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.float32)\n",
    "reference.to(device)\n",
    "\n",
    "dataset_name = [\"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"Stanford_Cars\", \"SUN397\", \"SVHN\"]\n",
    "fine_tuned_name = [\"tanganke/clip-vit-base-patch32_dtd\", \"tanganke/clip-vit-base-patch32_eurosat\", \"tanganke/clip-vit-base-patch32_gtsrb\", \"tanganke/clip-vit-base-patch32_mnist\", \"tanganke/clip-vit-base-patch32_resisc45\", \"tanganke/clip-vit-base-patch32_stanford-cars\", \"tanganke/clip-vit-base-patch32_sun397\", \"tanganke/clip-vit-base-patch32_svhn\"]\n",
    "\n",
    "custom_models = False\n",
    "num_models = 5 if custom_models else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89212a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Matrix(torch.nn.Module):\n",
    "    def __init__(self, model, W=None, transform_stage=-1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.transform_stage = transform_stage\n",
    "    \n",
    "    def vision_manipulation(self, pixel_values):\n",
    "        hidden_states = self.model.vision_model.embeddings(pixel_values)\n",
    "        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n",
    "\n",
    "        # Following CLIPEncoder\n",
    "        for i, encoder_layer in enumerate(self.model.vision_model.encoder.layers):\n",
    "            encoder_output = encoder_layer(hidden_states, attention_mask=None, causal_attention_mask=None, output_attentions=False)\n",
    "            hidden_states = encoder_output[0]\n",
    "            if i == self.transform_stage:\n",
    "                if self.W is not None:\n",
    "                    cls = hidden_states[:, 0, :]\n",
    "                    # original multiplied by matrix\n",
    "                    cls = cls @ self.W\n",
    "                    hidden_states[:, 0, :] = cls\n",
    "                break\n",
    "        \n",
    "        pooled_output = hidden_states[:, 0, :]\n",
    "        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, pixel_values, all_label_embeds):\n",
    "        image_embeds = self.model.visual_projection(self.vision_manipulation(pixel_values))\n",
    "        image_embeds /= image_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = image_embeds @ all_label_embeds.T\n",
    "        logits = logits * self.model.logit_scale.exp()\n",
    "        pred = logits.argmax(dim=1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7dfc7",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb79eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Task_Matrix(model=copy.deepcopy(reference))\n",
    "base.eval().to(device)\n",
    "\n",
    "os.makedirs(\"fine_tuned_acc\", exist_ok=True)\n",
    "for i, name in enumerate(dataset_name):\n",
    "    fine_tuned_acc = {j: 0 for j in range(num_models)}\n",
    "    base_acc = 0\n",
    "    total = 0\n",
    "    fine_tuned_models = {}\n",
    "    for j in range(num_models):\n",
    "        if custom_models:\n",
    "            fine_tuned = copy.deepcopy(reference)\n",
    "            fine_tuned.load_state_dict(torch.load(f\"fine_tuned/{name}/CLIP_{name}_{j}.pt\"))\n",
    "            fine_tuned = Task_Matrix(model=fine_tuned)\n",
    "            fine_tuned.eval().to(device)\n",
    "            fine_tuned_models[j] = fine_tuned\n",
    "        else:\n",
    "            vision_model = CLIPVisionModel.from_pretrained(f\"{fine_tuned_name[i]}\")\n",
    "            fine_tuned = copy.deepcopy(reference)\n",
    "            fine_tuned.vision_model.load_state_dict(vision_model.vision_model.state_dict())\n",
    "            fine_tuned = Task_Matrix(model=fine_tuned)\n",
    "            fine_tuned.eval().to(device)\n",
    "            fine_tuned_models[j] = fine_tuned\n",
    "\n",
    "    test_dataset = load_from_disk(f\"data/{name}/test\")\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    all_label_embeds = torch.load(f\"data/{name}/all_label_embeds.pt\").to(device)\n",
    "\n",
    "    test_progress = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "            base_preds = base(pixel_values=pixel_values, all_label_embeds=all_label_embeds)\n",
    "            base_acc += (base_preds == labels).sum().item()\n",
    "\n",
    "            for j in range(num_models):\n",
    "                fine_tuned_preds = fine_tuned_models[j](pixel_values=pixel_values, all_label_embeds=all_label_embeds)\n",
    "                fine_tuned_acc[j] += (fine_tuned_preds == labels).sum().item()\n",
    "            \n",
    "            total += len(labels)\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    base_acc /= total\n",
    "    data[\"Base_Acc\"] = [base_acc]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"\\tBase Accuracy: {base_acc}\")\n",
    "    for j in range(num_models):\n",
    "        fine_tuned_acc[j] /= total\n",
    "        print(f\"\\tFine-Tuned Accuracy {j}: {fine_tuned_acc[j]}\")\n",
    "        data[f\"Fine-Tuned_Acc_{j}\"] = [fine_tuned_acc[j]]\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    split_folder = \"custom\" if custom_models else \"tanganke\"\n",
    "    full_save = f\"fine_tuned_acc/{split_folder}\"\n",
    "    os.makedirs(full_save, exist_ok=True)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_json(f\"{full_save}/{name}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

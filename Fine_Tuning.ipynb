{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48c6ac5",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4202128",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision \n",
    "!pip install transformers datasets tqdm ipywidgets hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel\n",
    "from datasets import load_from_disk\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d1e9c",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "reference = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.float32)\n",
    "reference.to(device)\n",
    "\n",
    "dataset_name = [\"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"Stanford_Cars\", \"SUN397\", \"SVHN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34097157",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "patience = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6491b9",
   "metadata": {},
   "source": [
    "## Fine-Tune Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46560925",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dataset_name:\n",
    "    train_dataset = load_from_disk(f\"data/{name}/train\")\n",
    "    val_dataset = load_from_disk(f\"data/{name}/val\")\n",
    "\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"input_ids\", \"attention_mask\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True, persistent_workers=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    os.makedirs(f\"fine_tuned/{name}\", exist_ok=True)\n",
    "    for indice in range(5):\n",
    "        model = copy.deepcopy(reference)\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.vision_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.vision_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) # remove if not helping\n",
    "\n",
    "        loss_stop_counter = 0\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = 0\n",
    "            model.train()\n",
    "            train_progress = tqdm(train_loader, total=len(train_loader))\n",
    "\n",
    "            for batch in train_progress:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, return_loss=True)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss  / len(train_loader)\n",
    "            \n",
    "            val_loss = 0\n",
    "            model.eval()\n",
    "            val_progress = tqdm(val_loader, total=len(val_loader))\n",
    "            with torch.no_grad():\n",
    "                for batch in val_progress:\n",
    "                    pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                    input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "                    outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, return_loss=True)\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            scheduler.step() # remove if not helping\n",
    "            \n",
    "            print(f\"EPOCH: {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                loss_stop_counter = 0\n",
    "                torch.save(model.state_dict(), f\"fine_tuned/{name}/CLIP_{name}_{indice}.pt\")\n",
    "                print(f\"Current Best EPOCH: {epoch} | Loss: {best_val_loss}\")\n",
    "            else:\n",
    "                loss_stop_counter += 1\n",
    "                print(f\"Best EPOCH: {epoch - loss_stop_counter} | Loss: {best_val_loss}\")\n",
    "\n",
    "            if loss_stop_counter >= patience:\n",
    "                print(f\"Early stoppage on epoch {epoch}!\")\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

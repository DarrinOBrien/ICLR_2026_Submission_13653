{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934a498c",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch torchvision\n",
    "!pip install transformers datasets tqdm ipywidgets numpy pandas hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U torch torchvision\n",
    "%pip install transformers datasets tqdm ipywidgets numpy pandas hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPVisionModel\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda21946",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "reference = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.float32)\n",
    "reference.to(device)\n",
    "\n",
    "dataset_name = [\"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"Stanford_Cars\", \"SUN397\", \"SVHN\"]\n",
    "fine_tuned_name = [\"tanganke/clip-vit-base-patch32_dtd\", \"tanganke/clip-vit-base-patch32_eurosat\", \"tanganke/clip-vit-base-patch32_gtsrb\", \"tanganke/clip-vit-base-patch32_mnist\", \"tanganke/clip-vit-base-patch32_resisc45\", \"tanganke/clip-vit-base-patch32_stanford-cars\", \"tanganke/clip-vit-base-patch32_sun397\", \"tanganke/clip-vit-base-patch32_svhn\"]\n",
    "\n",
    "custom_models = False # False if using Tanganke's models\n",
    "type = 1 # How many \n",
    "num_models = 5 if custom_models else 1\n",
    "layers = [i for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe348f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Matrix(torch.nn.Module):\n",
    "    def __init__(self, model, W=None, transform_stage=-1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.transform_stage = transform_stage\n",
    "    \n",
    "    def vision_manipulation(self, pixel_values):\n",
    "        hidden_states = self.model.vision_model.embeddings(pixel_values)\n",
    "        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n",
    "\n",
    "        # Following CLIPEncoder\n",
    "        for i, encoder_layer in enumerate(self.model.vision_model.encoder.layers):\n",
    "            encoder_output = encoder_layer(hidden_states, attention_mask=None, causal_attention_mask=None, output_attentions=False)\n",
    "            hidden_states = encoder_output[0]\n",
    "            if i == self.transform_stage:\n",
    "                if self.W is not None:\n",
    "                    cls = hidden_states[:, 0, :]\n",
    "                    # original multiplied by matrix\n",
    "                    cls = cls @ self.W\n",
    "                    hidden_states[:, 0, :] = cls\n",
    "                break\n",
    "        \n",
    "        pooled_output = hidden_states[:, 0, :]\n",
    "        pooled_output = self.model.vision_model.post_layernorm(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, pixel_values, all_label_embeds):\n",
    "        image_embeds = self.model.visual_projection(self.vision_manipulation(pixel_values))\n",
    "        image_embeds /= image_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = image_embeds @ all_label_embeds.T\n",
    "        logits = logits * self.model.logit_scale.exp()\n",
    "        pred = logits.argmax(dim=1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingHooks(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.CLS = []\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        self.CLS = []\n",
    "        hidden_states = self.model.vision_model.embeddings(pixel_values)\n",
    "        hidden_states = self.model.vision_model.pre_layrnorm(hidden_states)\n",
    "\n",
    "        # Following CLIPEncoder\n",
    "        for encoder_layer in self.model.vision_model.encoder.layers:\n",
    "            encoder_output = encoder_layer(hidden_states, attention_mask=None, causal_attention_mask=None, output_attentions=False)\n",
    "            hidden_states = encoder_output[0]\n",
    "            self.CLS.append(hidden_states[:, 0, :])\n",
    "        \n",
    "        return self.CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3839f0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1606fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_matrix_solver(base_hook, fine_tuned_hook, train_loader):\n",
    "    base_embeddings = {l: [] for l in layers}\n",
    "    fine_tuned_embeddings = []\n",
    "\n",
    "    train_progress = tqdm(train_loader, total=len(train_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch in train_progress:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "\n",
    "            base_embed = base_hook(pixel_values)\n",
    "            fine_tuned_embed = fine_tuned_hook(pixel_values)[-1] # Last Layer\n",
    "\n",
    "            for l in layers:\n",
    "                base_embeddings[l].append(base_embed[l].float().cpu())\n",
    "            fine_tuned_embeddings.append(fine_tuned_embed.float().cpu())\n",
    "    \n",
    "    W = {}\n",
    "\n",
    "    Z1_last = torch.cat(fine_tuned_embeddings)\n",
    "    Z1 = Z1_last.cpu().numpy()\n",
    "\n",
    "    for l, val in base_embeddings.items():\n",
    "        val = torch.cat(val)\n",
    "        val = val.cpu().numpy()\n",
    "        W[l], resid, rank, s = np.linalg.lstsq(val, Z1, rcond=None)\n",
    "        \n",
    "    del fine_tuned_hook\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a107c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_model(W):\n",
    "    task_matrix_augmented = {}\n",
    "    for l in layers:\n",
    "        aug = Task_Matrix(model=copy.deepcopy(reference), W=W[l], transform_stage=l)\n",
    "        aug.eval().to(device)\n",
    "        task_matrix_augmented[l] = aug\n",
    "    return task_matrix_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(augmented_models, test_loader, all_label_embeds):\n",
    "    task_matrix_acc = {i: 0 for i in layers}\n",
    "    total = 0\n",
    "\n",
    "    progress = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch in progress:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "            for model_num in augmented_models.keys():\n",
    "                pred = augmented_models[model_num](pixel_values=pixel_values, all_label_embeds=all_label_embeds)\n",
    "                task_matrix_acc[model_num] += (pred == labels).sum().item()\n",
    "            \n",
    "            total += len(labels)\n",
    "    \n",
    "    for l in layers:\n",
    "        task_matrix_acc[l] /= total\n",
    "    \n",
    "    return task_matrix_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcc41d",
   "metadata": {},
   "source": [
    "## Augmentation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(dataset_name):\n",
    "    base_hook = EmbeddingHooks(model=copy.deepcopy(reference))\n",
    "    base_hook.eval().to(device)\n",
    "\n",
    "    train_dataset = load_from_disk(f\"data/{name}/train\") # Can look into minimal data if necessary with the 'label' column\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"input_ids\", \"attention_mask\"])\n",
    "    if not custom_models:\n",
    "        val_dataset = load_from_disk(f\"data/{name}/val\")\n",
    "        val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"input_ids\", \"attention_mask\"])\n",
    "        train_dataset = concatenate_datasets([train_dataset, val_dataset])\n",
    "    num_train_img = len(train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    test_dataset = load_from_disk(f\"data/{name}/test\")\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    all_label_embeds = torch.load(f\"data/{name}/all_label_embeds.pt\").to(device)\n",
    "\n",
    "    print(name)\n",
    "    os.makedirs(f\"results/{name}\", exist_ok=True)\n",
    "    for models in range(num_models):\n",
    "        if custom_models:\n",
    "            fine_tuned = copy.deepcopy(reference)\n",
    "            fine_tuned.load_state_dict(torch.load(f\"fine_tuned/{name}/CLIP_{name}_{models}.pt\"))\n",
    "        else:\n",
    "            vision_model = CLIPVisionModel.from_pretrained(f\"{fine_tuned_name[i]}\")\n",
    "            fine_tuned = copy.deepcopy(reference)\n",
    "            fine_tuned.vision_model.load_state_dict(vision_model.vision_model.state_dict())\n",
    "        \n",
    "        fine_tuned_hook = EmbeddingHooks(model=copy.deepcopy(fine_tuned))\n",
    "        fine_tuned_hook.eval().to(device)\n",
    "\n",
    "        W = task_matrix_solver(base_hook=base_hook, fine_tuned_hook=fine_tuned_hook, train_loader=train_loader)\n",
    "        task_matrix_models = augment_model(W)\n",
    "        task_matrix_acc = evaluate(augmented_models=task_matrix_models, test_loader=test_loader, all_label_embeds=all_label_embeds)\n",
    "\n",
    "        express = f\"Model {models}\" if custom_models else f\"Tanganke's Model\"\n",
    "        print(f\"Task Matrix Accuracy | {express}\")\n",
    "        for layer, acc in task_matrix_acc.items():\n",
    "            print(f\"\\t Layer {layer}: {acc}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        results_path = f\"results/{name}/custom/{models}\" if custom_models else f\"results/{name}/tanganke\"\n",
    "        file_name = f\"{num_train_img}_accuracy.json\"\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        data = {\n",
    "            \"Task_Matrix_Accuracy\": [task_matrix_acc[l] for l in layers]\n",
    "        }\n",
    "        df = pd.DataFrame(task_matrix_acc, index=[0])\n",
    "        df.to_json(f\"{results_path}/{file_name}\", orient=\"records\", indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
